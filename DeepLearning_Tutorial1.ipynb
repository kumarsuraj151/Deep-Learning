{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kumarsuraj151/Deep-Learning/blob/main/DeepLearning_Tutorial1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Learning Tutorial 1\n",
        "## Date: $12^{th}$ January 2023\n",
        "## Suraj kumar"
      ],
      "metadata": {
        "id": "qye2qbjvv54l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What are notebooks?\n",
        "\n",
        "* Help integrate code and text explanations for better understanding. \n",
        "\n",
        "* Used in industries throught for better presentation and explainability of code.\n",
        "\n",
        "* Have code cells and text cells (Markdown Cells)\n",
        "\n",
        "* Can use Colab Notebooks (which we will be using for the tutorial) or Jupyter Notebooks\n"
      ],
      "metadata": {
        "id": "W0cQdqUYwCST"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How are the Markdown cells useful?\n",
        "\n",
        "* Can use basic Markdown formatting for the following:\n",
        "    * lists\n",
        "        * ordered lists\n",
        "        * unordered lists\n",
        "        * nested lists\n",
        "    * linking images and URLs\n",
        "    * **bold** , *italics* and neater presentation\n",
        "    * Mathematical Formulaes\n",
        "\n",
        "* We can express a lot of the mathematics within our code using the notebook itself. We can use simple $\\LaTeX$. For example, the gradient descent rule is:\n",
        "\n",
        "  $\\theta_{n+1} \\leftarrow \\theta_n - \\eta \\frac{\\delta L}{\\delta \\theta}$. \n",
        "  \n",
        "* Another example is the ReLU function:\n",
        "$\n",
        "    ReLU(x) = \\max \\{0,x\\} = \n",
        "    \\begin{cases}\n",
        "    x & x\\geq 0\\\\\n",
        "    0 & otherwise\n",
        "    \\end{cases}\n",
        "$\n",
        "    \n",
        "* What this also means is you can turn in notebooks consisting of coding and theoretical answers all in one place!"
      ],
      "metadata": {
        "id": "uVqUDLoIwvD3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Features of Colab\n",
        "\n",
        "* Can use a CPU or a GPU (beware not to go beyond your alloted time, i.e switch off when not in use)\n",
        "* Can import datasets, and upload them directly to Google Collab( there is a disadvantage here, which we will tell you about), or import it from Google Drive (a recent disadvantage here too!) or even Github\n",
        "* Can share via Google Drive, and is easy to access and collaborate with others\n",
        "* Can even import necessary libraries, or even install them using pip!\n",
        "* Can run command line commands such as cd, pwd etc."
      ],
      "metadata": {
        "id": "uN8YFJynzKzQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing data\n",
        "\n",
        "* Colab has a runtime where in it saves all your variables, libraries imported, files uploaded, etc. However, after the runtime is over, all this is lost. This becomes inconvinient especially if you have to upload a large file everytime.\n",
        "\n",
        "* Alternative: Upload to Google Drive, mount the Drive, and import from there... (May cause an issue with the recent limit on Google Drive Storage ;))\n",
        "\n",
        "* Can also import from Github... May require SSH keys and/or other authentication and might become a bit complex!\n",
        "\n",
        "* How to mount your drive? Go to the Command Pallete at the bottom left, and choose the option to Mount Drive. Will require you to grant authentication and permissions!\n"
      ],
      "metadata": {
        "id": "IbAXEf11z8LJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "zMtLhnrbBe3D"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A revision of Numpy\n",
        "\n",
        "The documentation is amazing! And so are the online resources!"
      ],
      "metadata": {
        "id": "MT323mLNsiRE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "55r2-hwFtBx4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# difference between np.linspace and np.arange\n",
        "\n",
        "linspace = np.linspace(0,10,10 , endpoint = False) # uses number of equally seperated points\n",
        "arange = np.arange(0,10,1)  # uses distance between two points\n",
        "\n",
        "print(\"Linspace -> \" , linspace)\n",
        "print(\"Arange -> \" , arange)\n",
        "\n",
        "# other important matrices\n",
        "identity = np.identity(2)\n",
        "print(\"Identity matrix -> \\n\" , identity)"
      ],
      "metadata": {
        "id": "MqO1pWU9shmP",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1dc163ee-6737-4a1c-8802-775e5462dd04"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linspace ->  [0. 1. 2. 3. 4. 5. 6. 7. 8. 9.]\n",
            "Arange ->  [0 1 2 3 4 5 6 7 8 9]\n",
            "Identity matrix -> \n",
            " [[1. 0.]\n",
            " [0. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# simple shape operations\n",
        "\n",
        "print(\"The shape is \" , linspace.shape)\n",
        "print(\"The number of elements are \" , linspace.size)\n"
      ],
      "metadata": {
        "id": "WjnuDTZot7--",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4aa01f0-39d6-4dd0-c104-a0d02dd846f1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The shape is  (10,)\n",
            "The number of elements are  10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# using broadcasting\n",
        "print(1 + linspace)\n",
        "\n",
        "print(\"Why broadcasting is important to know is because it prevents needless copies of data, and allows for looping to happen in C instead of Python\")"
      ],
      "metadata": {
        "id": "pxFUXyl_tYXr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc1ce21d-08df-462f-8625-973eec563ea4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 1.  2.  3.  4.  5.  6.  7.  8.  9. 10.]\n",
            "Why broadcasting is important to know is because it prevents needless copies of data, and allows for looping to happen in C instead of Python\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# simple numpy operations that apply to the entire array\n",
        "print(\"Squaring -> \\n\" , np.power(linspace , 2))\n",
        "print(\"Exponentiation -> \\n\" , np.exp(linspace))\n",
        "print(\"Logarithms -> \\n\" , np.log2(linspace + 1)) ## would have encountered a NaN if we wouldnt have added the 1"
      ],
      "metadata": {
        "id": "-NTl3JDbtdSJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23da3f2b-8185-48d4-9c89-76f962f1c12c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Squaring -> \n",
            " [ 0.  1.  4.  9. 16. 25. 36. 49. 64. 81.]\n",
            "Exponentiation -> \n",
            " [1.00000000e+00 2.71828183e+00 7.38905610e+00 2.00855369e+01\n",
            " 5.45981500e+01 1.48413159e+02 4.03428793e+02 1.09663316e+03\n",
            " 2.98095799e+03 8.10308393e+03]\n",
            "Logarithms -> \n",
            " [0.         1.         1.5849625  2.         2.32192809 2.5849625\n",
            " 2.80735492 3.         3.169925   3.32192809]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# getting into some matrices\n",
        "\n",
        "matrix = np.reshape(linspace , (2,5))    # two rows and five columns\n",
        "print(\"Matrix -> \\n\" , matrix)\n",
        "matrix2 = np.reshape(linspace , (5 , -1)) # if you are too lazy to calculate the number of rows or columns, let numpy do it! \n",
        "print(\"Matrix2 -> \\n\" , matrix2)"
      ],
      "metadata": {
        "id": "uvtIYkyiuszr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6ed949a-1fe7-402a-ca80-370187ba9bcc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix -> \n",
            " [[0. 1. 2. 3. 4.]\n",
            " [5. 6. 7. 8. 9.]]\n",
            "Matrix2 -> \n",
            " [[0. 1.]\n",
            " [2. 3.]\n",
            " [4. 5.]\n",
            " [6. 7.]\n",
            " [8. 9.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# slicing and dicing\n",
        "print(\"First row of matrix is -> \\n\" , matrix[0 , :])\n",
        "print(\"a_12 of matrix is -> \\n\" , matrix[0][1])\n",
        "print(\"Third column of matrix is ->\\n\" , matrix[: , 2])\n",
        "print(\"All the rows from column 2 to column 4 are -> \\n\" , matrix[: , 1:3])\n",
        "print(\"Reversing all the rows of matrix -> \\n\" , matrix[: , ::-1])\n",
        "print(\"In general reversing can be done using the ::-1 format. For example, reversing linspace ->\\n\" , linspace[::-1])\n",
        "print(\"Take Care, Python follows ZERO-indexing\")"
      ],
      "metadata": {
        "id": "jj0c4-M-0CDq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "535b2c11-f5cb-49c2-c2de-b47e5bb0008f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First row of matrix is -> \n",
            " [0. 1. 2. 3. 4.]\n",
            "a_12 of matrix is -> \n",
            " 1.0\n",
            "Third column of matrix is ->\n",
            " [2. 7.]\n",
            "All the rows from column 2 to column 4 are -> \n",
            " [[1. 2.]\n",
            " [6. 7.]]\n",
            "Reversing all the rows of matrix -> \n",
            " [[4. 3. 2. 1. 0.]\n",
            " [9. 8. 7. 6. 5.]]\n",
            "In general reversing can be done using the ::-1 format. For example, reversing linspace ->\n",
            " [9. 8. 7. 6. 5. 4. 3. 2. 1. 0.]\n",
            "Take Care, Python follows ZERO-indexing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# transposing matrices\n",
        "print(\"Transpose can be achoeved using np.transpose() or simply .T -> \\n\",np.transpose(matrix))\n",
        "print(\"Like we said another way using .T-> \\n\" , matrix.T)\n",
        "print(\"For more than 2D matrices, be careful of which dimension to transpose along\")"
      ],
      "metadata": {
        "id": "rzpOSIivvOD-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67f79830-6193-4d72-8155-25c8524a496f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transpose can be achoeved using np.transpose() or simply .T -> \n",
            " [[0. 5.]\n",
            " [1. 6.]\n",
            " [2. 7.]\n",
            " [3. 8.]\n",
            " [4. 9.]]\n",
            "Like we said another way using .T-> \n",
            " [[0. 5.]\n",
            " [1. 6.]\n",
            " [2. 7.]\n",
            " [3. 8.]\n",
            " [4. 9.]]\n",
            "For more than 2D matrices, be careful of which dimension to transpose along\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# matrix multiplication and dot products\n",
        "print(\"Matrix Multiplication -> of matrix and matrix2 -> \\n\" , np.matmul(matrix , matrix2))\n",
        "print(\"Simply use @ for matrix Multiplication -> \\n \" , matrix @ matrix2)\n",
        "print(\"Element wise multiplication -> \\n\" ,np.multiply(matrix , matrix))\n",
        "print(\"Dot product -> \\n \" , np.dot(linspace , linspace))"
      ],
      "metadata": {
        "id": "VLcvkYFrvlmN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43b53541-85fe-4333-a5c3-ba4a9b3d9046"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix Multiplication -> of matrix and matrix2 -> \n",
            " [[ 60.  70.]\n",
            " [160. 195.]]\n",
            "Simply use @ for matrix Multiplication -> \n",
            "  [[ 60.  70.]\n",
            " [160. 195.]]\n",
            "Element wise multiplication -> \n",
            " [[ 0.  1.  4.  9. 16.]\n",
            " [25. 36. 49. 64. 81.]]\n",
            "Dot product -> \n",
            "  285.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# and the normal functions\n",
        "\n",
        "print(\"Sum of matrix -> \\n\" , np.sum(matrix))\n",
        "print(\"Sum of each column -> \\n\" , np.sum(matrix , axis = 0))\n",
        "print(\"Average of matrix -> \\n\" , np.mean(matrix))\n",
        "print(\"Average of matrix along each row \\n\" , np.mean(matrix , axis = 1))"
      ],
      "metadata": {
        "id": "9gGOiZzFzMqI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68971512-9180-456c-f673-5880f9a88d36"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sum of matrix -> \n",
            " 45.0\n",
            "Sum of each column -> \n",
            " [ 5.  7.  9. 11. 13.]\n",
            "Average of matrix -> \n",
            " 4.5\n",
            "Average of matrix along each row \n",
            " [2. 7.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Getting started with Torch\n",
        "Now that we have gotten ourseleves going with NumPy, switching to PyTorch ain't that difficult! Moreover, the documentation is really great!"
      ],
      "metadata": {
        "id": "S9ArzwwH025X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "O-KRNgqp1APX"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\textbf{Tensor}$: It is a multi dimensional matrix containing elements of single data type."
      ],
      "metadata": {
        "id": "OGN9Onc8dT1u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# making a tensor\n",
        "\n",
        "# 1. from a list\n",
        "t1 = torch.tensor([1,2,3,4,5])\n",
        "print(\"Type of T1 is -> \" , type(t1))\n",
        "\n",
        "# 2. from numpy array\n",
        "t2 = torch.tensor(linspace)\n",
        "print(\"T2 is {} and its type is {}\".format(t2 , type(t2)))"
      ],
      "metadata": {
        "id": "XMxAAm_rFJ5A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07dc29c7-6491-41cc-8b39-cadc20467946"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type of T1 is ->  <class 'torch.Tensor'>\n",
            "T2 is tensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.], dtype=torch.float64) and its type is <class 'torch.Tensor'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reshaping and resizing\n",
        "t2 = t2.reshape((2,5))\n",
        "print(\"T2 is -> \\n\" , t2)\n",
        "\n",
        "# transposing\n",
        "print(\"The transposed tensor is -> \\n\" , t2.T)"
      ],
      "metadata": {
        "id": "qEUD3loIpDUQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da56ee9a-9c7c-4179-aa81-ff53c4689011"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "T2 is -> \n",
            " tensor([[0., 1., 2., 3., 4.],\n",
            "        [5., 6., 7., 8., 9.]], dtype=torch.float64)\n",
            "The transposed tensor is -> \n",
            " tensor([[0., 5.],\n",
            "        [1., 6.],\n",
            "        [2., 7.],\n",
            "        [3., 8.],\n",
            "        [4., 9.]], dtype=torch.float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ways of defining a tensor \n",
        "\n",
        "# with zeros or ones\n",
        "z = torch.zeros(5,4)\n",
        "o = torch.ones(5,2)\n",
        "print(z)\n",
        "print(o)\n",
        "\n",
        "# with \"rand\" ftn - Values are generated between [0,1)\n",
        "tensor1 = torch.rand(3,3)\n",
        "print(\"Tensor 1: \\n\",tensor1)\n",
        "\n",
        "tensor2 = torch.rand(3,3)\n",
        "print(\"Tensor 2: \\n\",tensor2)\n",
        "\n",
        "# What is the use of this randomly generated values then? -> Might help with generating your weights for a neural network...\n",
        "# Why do all zero initialised weights not work?\n",
        "\n",
        "# We can then convert these to values between [a,b] using: a + (b-a)*x\n",
        "# Example to convert into values between 5 and 10:\n",
        "print(\"Tensor 2 between 5 and 10 (uses broadcasting) -> \\n\" , 5  + (10-5)*tensor2)"
      ],
      "metadata": {
        "id": "XUA_IdcJcE9u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bb58cbb-c6fd-4614-ddd3-7f580a4ed792"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0.]])\n",
            "tensor([[1., 1.],\n",
            "        [1., 1.],\n",
            "        [1., 1.],\n",
            "        [1., 1.],\n",
            "        [1., 1.]])\n",
            "Tensor 1: \n",
            " tensor([[0.2687, 0.6671, 0.1249],\n",
            "        [0.8288, 0.5926, 0.0161],\n",
            "        [0.6530, 0.3799, 0.7336]])\n",
            "Tensor 2: \n",
            " tensor([[0.8634, 0.7077, 0.9060],\n",
            "        [0.0038, 0.8388, 0.4123],\n",
            "        [0.0134, 0.5727, 0.1289]])\n",
            "Tensor 2 between 5 and 10 (uses broadcasting) -> \n",
            " tensor([[9.3170, 8.5383, 9.5302],\n",
            "        [5.0190, 9.1941, 7.0615],\n",
            "        [5.0668, 7.8635, 5.6443]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Slicing is similar to NumPy\n",
        "\n",
        "print(\"Element 1 is \",tensor2[0][0])\n",
        "print(\"Row 1 is\", tensor2[0 , :])\n",
        "print(\"Column 2 is \" , tensor2[: , 1])\n"
      ],
      "metadata": {
        "id": "Nk8OlgdrrLQu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecc9c11b-ec91-4cde-c9e3-ecdb7bc51e3d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Element 1 is  tensor(0.8634)\n",
            "Row 1 is tensor([0.8634, 0.7077, 0.9060])\n",
            "Column 2 is  tensor([0.7077, 0.8388, 0.5727])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# inplace operations -> inplace refers to changes being reflected in the original tensor\n",
        "# two ways\n",
        "\n",
        "t = torch.tensor([1,2,3,4])\n",
        "t.add(2)\n",
        "print(\"T without inplace operation -> \\n\" , t)\n",
        "\n",
        "# way 1\n",
        "t = t.add(2)\n",
        "print(\"T with inplace operation -> \\n\" , t)\n",
        "\n",
        "# way 2 -> underscore\n",
        "\n",
        "# again generating t\n",
        "t = torch.tensor([1,2,3,4])\n",
        "t.add_(2)\n",
        "print(\"T with inplace operation using underscore-> \\n\" , t)"
      ],
      "metadata": {
        "id": "hsK3G696tEkv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "132d4d77-f2bc-4210-f6e7-92eb584f68aa"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "T without inplace operation -> \n",
            " tensor([1, 2, 3, 4])\n",
            "T with inplace operation -> \n",
            " tensor([3, 4, 5, 6])\n",
            "T with inplace operation using underscore-> \n",
            " tensor([3, 4, 5, 6])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# copying tensors -> note that two different tensors might share the same memory\n",
        "t = torch.tensor([1,2,3,4])\n",
        "print(\"Initial t = \\n\" , t)\n",
        "a = t\n",
        "a[0] = 10\n",
        "print(\"Modifying a results in modification of t -> \\n\" , t)\n",
        "\n",
        "\n",
        "# regenerating t\n",
        "t = torch.tensor([1,2,3,4])\n",
        "b = t.clone()\n",
        "b[0] = 10\n",
        "print(\"Modifying b does not result in modification of t -> \\n\" , t)\n",
        "print(\"However, b does get modified -> \\n\" , b)"
      ],
      "metadata": {
        "id": "LKNuvYwDtQ26",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f23dc6f-a592-44c8-cbc6-90b29ce26bfc"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial t = \n",
            " tensor([1, 2, 3, 4])\n",
            "Modifying a results in modification of t -> \n",
            " tensor([10,  2,  3,  4])\n",
            "Modifying b does not result in modification of t -> \n",
            " tensor([1, 2, 3, 4])\n",
            "However, b does get modified -> \n",
            " tensor([10,  2,  3,  4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# converting 0D into scalar\n",
        "print(\"The sum as a 0D array is -> \", t.sum())\n",
        "\n",
        "# to extract the value, we can use .item()\n",
        "print(\"The scalar sum is -> \", t.sum().item())"
      ],
      "metadata": {
        "id": "gdx_UoY_vlXg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ff917ec-8ff5-45a8-ff7b-42b24cda6ebe"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The sum as a 0D array is ->  tensor(10)\n",
            "The scalar sum is ->  10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# checking for device\n",
        "t = torch.tensor([i for i in range(6)]).reshape((3,2))\n",
        "print(\"t is -> \\n\" , t)\n",
        "print(\"Tensor t is on device\",t.device)\n",
        "\n",
        "# shifting to cuda\n",
        "# change runtime type to GPU\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "t = t.to(device)\n",
        "print(\"The device is now \" , t.device)"
      ],
      "metadata": {
        "id": "fqcdzhy8wjog",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3aac1e4-9fff-469d-965f-b461a0a80fc4"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "t is -> \n",
            " tensor([[0, 1],\n",
            "        [2, 3],\n",
            "        [4, 5]])\n",
            "Tensor t is on device cpu\n",
            "The device is now  cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Similarities with NumPy\n",
        "\n",
        "print(\"Our tensor is -> \\n\" , t)\n",
        "print(\"Sum of all elements is -> \" , torch.sum(t).item())\n",
        "print(\"Sum of each column is -> \\n\" , torch.sum(t , axis = 0))\n",
        "\n",
        "# print(\"Mean is -> \" , torch.mean(t).item()) # will return error because of datatypes\n",
        "# because if input is long/int64, output might be float. So it requires float dtype\n",
        "print(\"Datatype is \" , t.dtype)\n",
        "t_float = t.float()\n",
        "print(\"Datatype is after conversion \" , t_float.dtype)\n",
        "print(\"Mean is -> \" , torch.mean(t_float).item()) \n",
        "print(\"Mean is -> \" , torch.mean(t , dtype = torch.float))\n"
      ],
      "metadata": {
        "id": "95upW4CUSRJB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "614f7079-b22d-4dda-d636-9de9e6ec51df"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our tensor is -> \n",
            " tensor([[0, 1],\n",
            "        [2, 3],\n",
            "        [4, 5]], device='cuda:0')\n",
            "Sum of all elements is ->  15\n",
            "Sum of each column is -> \n",
            " tensor([6, 9], device='cuda:0')\n",
            "Datatype is  torch.int64\n",
            "Datatype is after conversion  torch.float32\n",
            "Mean is ->  2.5\n",
            "Mean is ->  tensor(2.5000, device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise : To obtain the real part of the DFT of a black and white image in 1D\n",
        "\n",
        "Suppose we have a sequence $\\{x_n\\} = x_0 , x_1 , \\ldots x_{N-1}$ then the Discrete Fourier Transform is another sequence  $\\{X_n\\} = X_0 , X_1 , \\ldots X_{N-1}$ given by:\n",
        "\n",
        "$X_k = \\sum\\limits_{n=0}^{N-1} x_n e^{-\\frac{i2\\pi kn}{N}} = \\sum\\limits_{n=0}^{N-1} x_n \\left[\\cos\\left(\\frac{2\\pi kn}{N}\\right) - i\\sin\\left(\\frac{2\\pi kn}{N}\\right)\\right]$\n",
        "\n",
        "Follow the instructions to perform a DFT"
      ],
      "metadata": {
        "id": "5P4fWB3hx8Rn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Generating data\n",
        "#################\n",
        "##             ##\n",
        "## DONOT EDIT ##\n",
        "##             ##\n",
        "#################\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "list = np.array([103,118,118,140,122,144,157,169,187,192,188,190,185,195,198,210,232,235,223,220,212,227,239,240,232,218,201,187,181,190,200,208,210,201,177,154,180,145,117,119,129,127,119,117,89,105,95,72,120,110,102,111,105,127,146,164,168,166,174,178,183,194,205,220,222,216,216,214,218,234,232,240,240,223,203,192,191,192,181,180,160,137,148,176,172,145,135,116,112,129,133,116,106,110,85,116,98,62,126,118,113,122,130,133,141,149,156,159,162,173,190,204,217,206,202,210,218,219,220,224,230,225,235,234,208,190,193,197,185,190,170,198,131,166,165,132,145,140,109,101,119,100,81,103,74,73,100,63,79,112,99,111,124,132,146,154,166,181,199,214,224,228,202,212,227,233,224,211,211,220,223,229,221,187,170,191,197,172,176,190,195,194,162,127,134,128,88,111,99,80,92,97,81,73,67,51,57,49,86,118,102,112,126,136,136,152,171,181,183,179,171,165,198,207,217,217,208,203,214,230,232,182,174,211,224,214,214,224,196,131,131,158,133,106,116,64,99,120,122,103,99,105,90,64,57,56,49,51,71,81,112,116,123,131,166,177,180,170,163,175,202,223,206,202,199,200,203,208,215,220,198,226,228,183,149,165,173,148,172,174,114,115,92,101,134,204,163,148,140,130,110,93,77,58,46,64,45,47,52,55,107,108,115,125,109,137,169,184,186,184,181,177,200,204,209,214,217,215,208,202,214,198,187,180,182,202,207,190,120,120,143,193,162,181,202,174,158,127,102,85,64,48,36,21,22,38,10,20,43,68,100,105,121,138,170,166,157,154,166,183,189,185,180,194,206,206,202,206,213,218,209,184,160,152,166,187,166,116,129,178,195,166,199,153,156,148,83,72,43,15,8,13,8,0,0,12,0,8,20,51,102,107,123,142,141,151,160,166,177,189,189,181,173,180,183,179,182,195,209,215,182,145,157,187,147,90,117,186,217,195,196,165,179,126,108,25,23,28,11,0,8,5,2,18,6,5,10,14,0,15,106,101,108,122,129,160,186,187,178,176,182,185,180,179,176,179,191,199,187,168,163,164,152,117,95,131,191,221,186,204,162,110,63,18,14,21,6,7,1,20,36,2,0,50,42,1,0,11,4,12,102,109,121,131,137,158,162,161,177,184,169,155,170,179,183,179,176,176,169,160,140,143,130,79,73,197,218,199,187,121,41,12,96,96,0,17,14,11,0,42,185,18,6,13,13,0,14,35,2,4,92,108,126,139,118,132,134,135,151,160,156,156,164,171,175,174,175,173,163,150,167,137,109,113,153,179,166,112,39,10,30,19,21,33,3,14,0,21,0,0,39,1,26,0,10,6,19,58,28,15,101,107,110,110,128,140,146,153,162,155,147,154,168,170,169,169,171,171,165,155,156,141,110,114,144,84,70,19,21,5,41,25,0,19,15,12,17,21,8,17,0,0,30,16,8,13,14,72,51,25,108,107,104,101,116,125,133,145,155,148,149,168,169,169,166,162,161,163,167,169,152,171,140,97,85,39,32,18,17,15,19,7,22,32,10,22,17,0,0,28,31,0,0,21,6,13,0,69,55,29,101,110,124,136,142,142,135,134,136,129,136,162,153,158,163,162,157,156,162,170,163,170,139,89,63,71,33,42,4,2,17,24,24,12,0,13,3,0,4,3,22,11,3,6,2,9,0,66,55,32,107,113,126,137,146,154,154,153,152,137,130,143,142,151,165,174,171,163,163,169,171,157,144,122,82,84,25,43,51,5,23,41,4,1,22,4,14,9,12,9,3,15,15,10,0,5,8,71,55,35,113,112,113,114,97,119,133,143,156,153,146,151,150,151,160,173,179,175,173,175,188,175,175,163,120,85,56,51,57,10,1,10,0,13,38,12,12,0,4,12,0,0,1,5,0,1,23,70,44,31,99,109,117,122,133,143,135,126,135,144,149,157,160,148,145,158,172,176,177,180,190,178,165,149,132,90,101,48,42,60,25,4,17,10,0,0,4,18,20,0,12,4,10,1,5,0,28,61,29,25,107,114,127,137,122,127,131,136,147,159,163,161,161,167,169,166,166,170,173,173,166,194,197,168,153,148,107,50,39,53,54,23,0,11,17,0,4,0,0,6,11,7,0,0,5,14,26,28,18,14,118,112,110,110,118,124,129,135,146,159,164,162,168,172,174,175,179,183,179,171,193,181,177,180,168,143,131,135,77,50,56,77,61,26,11,13,3,7,8,3,0,0,2,10,5,0,0,9,14,27,116,117,122,127,129,133,136,140,149,160,164,162,172,170,166,167,175,181,177,167,157,165,174,171,159,144,136,133,102,82,59,41,37,56,58,36,20,18,11,9,22,38,37,26,37,18,23,32,25,35,113,120,131,140,133,136,137,139,147,158,162,160,172,167,159,154,160,170,174,170,157,185,203,198,193,192,181,162,143,94,69,75,69,57,54,54,75,73,61,49,53,61,44,17,23,20,47,62,44,47,117,116,116,117,122,125,127,130,140,153,159,158,167,168,166,160,161,169,175,176,191,182,175,175,168,153,145,144,162,127,102,89,69,54,49,41,13,39,55,47,43,55,68,70,52,53,77,91,78,75,114,118,122,125,122,127,130,134,144,155,160,158,154,164,171,171,170,172,173,171,160,162,160,161,176,194,196,186,169,176,162,120,97,115,118,89,82,93,92,77,66,71,81,85,101,94,89,96,106,102,114,122,131,137,126,133,141,147,156,165,165,160,153,160,165,168,172,176,174,169,185,197,191,173,172,180,164,133,133,114,134,166,147,103,90,101,88,80,71,72,81,90,92,90,75,84,73,82,117,118,126,120,112,106,118,128,140,151,163,171,169,163,167,164,160,160,170,182,183,177,181,157,155,178,183,163,159,174,171,160,166,169,145,123,120,121,113,113,117,119,114,107,110,118,83,114,108,110,140,127,121,117,122,133,122,121,128,145,159,164,163,162,162,177,180,170,170,183,186,177,163,174,175,162,158,165,168,162,166,168,161,161,142,135,159,157,128,139,111,136,131,99,115,88,98,121,105,134,136,115,125,121,125,134,127,127,136,152,160,153,142,135,150,161,176,187,191,186,178,172,173,182,181,169,165,173,176,171,175,165,146,150,146,142,146,119,140,126,118,143,124,109,142,127,114,146,123,131,135,127,124,120,122,128,135,129,129,138,147,151,152,155,151,145,150,164,168,158,154,159,178,183,179,167,162,167,169,164,175,173,158,149,121,116,155,168,155,141,153,154,122,126,145,122,126,166,140,135,146,146,121,119,120,122,124,119,119,130,143,154,165,174,185,177,175,182,186,186,189,195,173,174,167,155,148,150,149,145,133,180,177,145,143,185,192,124,159,162,164,128,113,136,130,107,134,164,143,146,166,163,120,121,120,119,106,112,127,146,158,158,155,155,144,154,162,165,172,179,178,170,189,187,181,172,168,168,167,164,171,133,134,195,195,148,150,164,148,150,127,99,121,137,124,141,151,151,137,156,180,173,115,118,117,113,113,119,133,151,162,164,165,168,164,175,178,169,168,177,179,173,161,158,155,152,153,153,151,149,156,173,171,162,144,149,178,169,141,133,115,138,167,140,130,168,171,144,141,166,179,180,118,122,120,113,125,125,133,146,157,165,176,186,186,187,185,177,168,166,173,181,179,175,173,174,175,173,170,167,168,164,155,170,178,177,170,130,153,145,145,186,172,134,158,164,173,143,161,181,174,185,129,135,132,123,117,123,137,152,159,158,158,162,161,164,181,197,189,170,168,182,174,169,166,167,167,161,155,151,166,175,168,165,151,149,162,141,172,166,165,180,121,118,196,164,161,143,181,195,171,189,134,132,128,125,135,113,125,146,142,146,164,168,168,167,165,166,172,181,190,196,180,179,175,172,173,171,157,142,151,154,156,155,153,157,168,178,173,180,194,140,121,157,160,173,128,179,198,172,173,211,131,127,123,121,121,119,136,157,160,160,163,163,163,160,163,178,193,195,180,164,186,186,184,182,186,190,186,177,175,178,179,175,168,162,161,162,156,186,135,141,179,154,150,162,167,165,170,187,205,208,133,137,140,141,120,137,147,154,162,156,145,143,150,162,173,173,165,164,175,187,173,174,172,167,167,172,175,173,155,155,153,150,144,138,134,131,156,153,143,173,164,145,172,157,175,168,181,209,212,192,138,136,134,132,124,151,152,148,165,168,160,166,178,158,141,147,169,187,188,182,180,183,180,170,163,163,165,164,171,168,165,166,169,173,176,178,180,142,186,172,129,161,175,162,170,188,205,208,199,193,148,144,139,134,140,164,157,142,151,151,142,148,145,164,182,183,172,168,180,195,178,184,186,178,170,169,169,168,160,159,159,163,168,173,176,176,184,170,170,141,154,178,156,174,190,201,203,195,196,207,149,150,147,144,131,153,165,170,182,191,195,202,166,161,160,170,184,186,174,160,161,171,177,176,175,178,179,178,175,177,180,181,179,174,169,166,162,165,144,167,169,154,185,188,200,201,202,200,193,190,145,140,134,130,157,157,164,161,144,135,135,131,159,168,174,168,157,156,168,182,174,181,186,186,188,192,193,190,178,181,182,178,170,164,161,162,163,163,163,187,163,158,208,192,189,192,197,198,189,183,155,156,156,155,150,140,155,166,152,156,174,176,166,167,168,170,172,175,177,179,185,187,186,182,180,182,180,175,182,182,179,171,164,166,176,185,192,207,190,142,181,215,173,184,184,181,174,175,194,214,151,156,155,153,145,157,165,164,160,159,160,159,173,175,174,169,167,169,176,181,196,188,180,183,191,196,193,187,199,184,168,158,154,158,173,189,173,152,151,176,194,187,179,182,171,194,185,177,202,216,172,161,145,132,146,155,162,160,154,149,144,139,150,174,182,172,177,198,200,184,165,182,198,198,185,175,178,185,156,175,186,185,193,201,184,155,156,150,161,188,203,194,179,174,201,192,177,187,207,199,159,154,150,147,157,161,164,165,167,169,169,167,183,167,170,190,192,173,168,181,185,189,191,188,183,181,185,190,180,196,197,178,170,181,188,181,171,178,187,188,181,178,185,196,186,171,164,185,205,196,146,152,163,172,179,173,165,159,158,161,163,163,151,172,171,150,158,189,191,166,196,191,186,186,190,194,193,191,206,183,172,186,197,188,171,162,196,196,189,178,174,180,187,189,158,164,173,188,204,208,168,166,167,168,171,167,164,164,166,169,172,173,178,149,145,172,178,160,162,186,161,177,196,204,199,190,186,186,153,177,208,213,183,156,172,209,192,185,180,186,202,204,178,146,174,178,193,207,210,209,174,171,168,165,162,165,171,177,179,176,174,174,170,185,183,165,164,178,174,154,168,174,183,193,198,198,193,189,207,188,165,161,185,211,211,195,186,195,204,206,200,187,171,159,196,178,191,215,211,198,163,167,168,168,172,172,172,169,158,146,140,141,167,152,155,177,180,166,171,193,196,185,176,178,190,200,200,196,201,194,186,189,205,216,206,188,197,206,212,201,177,162,173,193,190,175,189,206,198,197,168,169,164,158,161,161,162,162,159,160,171,183,168,165,159,154,151,155,166,177,180,192,201,196,184,178,186,196,196,186,194,216,218,201,198,211,208,188,175,178,180,175,176,183,181,190,206,198,185,206,175,173,167,160,157,156,155,158,165,169,162,153,171,166,158,153,153,158,165,170,181,191,199,197,195,196,196,195,198,217,218,197,186,195,199,192,186,194,189,174,175,189,189,176,195,195,198,199,195,195,178,177,170,163,162,177,183,170,151,145,155,166,122,139,156,161,160,167,187,206,201,196,185,175,178,191,203,207,195,207,212,204,201,203,195,181,173,179,183,185,186,192,198,202,200,193,189,193,198,202,172,171,165,159,170,157,144,140,148,155,152,145,169,171,176,182,186,186,183,180,196,193,186,181,182,186,181,173,199,192,183,180,185,191,189,184,209,206,207,207,195,181,182,194,197,185,178,187,201,210,180,180,177,173,159,161,159,154,150,153,163,170,163,158,158,169,185,191,184,173,164,177,194,209,221,226,218,207,196,181,170,171,177,181,183,186,168,170,174,178,177,179,188,198,185,176,174,188,205,215,173,175,175,174,166,163,161,160,161,162,162,163,174,179,180,173,162,156,159,164,185,192,193,187,180,176,172,168,188,186,192,201,201,190,182,181,207,210,202,186,183,194,196,188,181,180,186,200,212,217,165,166,168,169,169,151,140,152,173,180,165,148,155,157,161,164,170,179,190,199,176,186,193,193,194,202,212,216,198,201,205,205,199,191,187,187,174,183,178,164,170,192,198,187,192,194,202,212,217,217,174,174,175,176,155,167,181,182,170,163,173,187,178,168,159,158,163,166,162,157,177,183,187,186,187,190,190,187,201,203,196,184,178,183,188,189,201,204,199,187,181,182,181,176,206,205,207,213,216,216,162,160,160,160,173,172,172,172,167,159,158,160,163,174,182,179,168,166,175,187,182,179,172,169,177,190,195,194,183,191,191,182,181,187,184,173,167,171,182,191,190,187,193,205,214,207,203,207,212,215,159,154,154,158,185,188,181,166,164,172,171,160,175,176,180,184,186,184,179,174,157,162,176,190,191,184,184,190,200,196,189,180,175,178,187,194,196,185,182,193,201,200,203,210,208,211,213,214,213,213,163,159,162,169,149,163,172,172,175,178,168,152,165,165,165,168,171,172,170,168,172,166,167,175,181,181,182,187,183,178,171,168,170,174,178,180,191,188,190,200,208,213,218,223,221,219,217,215,213,213,160,154,157,164,150,159,163,164,169,178,176,167,162,161,160,163,168,173,175,175,192,180,173,179,190,195,195,194,194,188,184,185,190,194,193,190,183,191,198,202,208,217,223,223,217,212,209,211,216,221,159,155,154,156,161,163,161,157,162,171,173,170,160,159,160,165,172,177,180,181,172,167,165,172,183,187,185,181,190,189,187,188,189,189,188,187,185,199,208,204,206,215,219,215,209,204,201,206,216,224,165,167,162,156,146,151,156,159,162,164,159,152,156,157,159,165,171,175,175,174,165,167,168,169,173,178,182,183,183,187,190,187,182,179,183,188,193,207,214,210,209,216,218,213,215,209,204,205,210,214,164,167,159,147,147,147,147,149,154,159,161,161,162,163,164,168,173,175,174,172,180,183,180,171,167,175,187,194,182,190,196,194,186,185,193,202,193,201,207,207,209,213,214,211,212,207,203,203,207,209,161,163,155,143,161,154,148,147,152,159,166,171,164,162,162,164,169,173,175,175,174,178,174,161,155,161,171,176,171,178,184,184,182,183,190,197,198,195,197,204,208,207,207,209,205,204,202,203,206,211,165,167,163,154,155,155,158,165,166,161,153,150,152,149,146,148,154,162,168,171,175,182,182,172,167,170,171,167,177,180,184,188,191,194,197,200,210,200,199,209,214,208,205,210,212,211,208,205,205,208,166,170,173,173,172,160,145,137,137,142,146,147,144,143,142,145,150,154,157,158,161,167,173,173,169,166,168,172,171,180,188,189,192,199,203,204,212,197,196,203,203,209,214,207,211,207,204,209,217,219,189,190,176,160,153,153,152,149,146,144,144,144,142,150,156,153,145,143,152,162,154,160,167,170,170,174,182,188,170,180,187,187,188,196,206,211,218,206,208,211,204,203,202,191,203,204,209,215,219,216,180,169,160,156,142,148,153,151,144,139,138,140,142,136,133,139,150,157,156,152,168,170,169,165,159,158,162,166,177,184,185,179,176,181,190,194,194,192,204,213,211,216,219,210,205,210,218,223,222,212,163,153,154,159,156,155,152,146,140,139,142,146,144,134,127,134,150,161,158,151,152,155,159,161,162,165,170,174,194,196,194,190,190,196,198,197,203,203,213,216,207,210,213,203,212,212,215,218,216,206,174,177,168,157,168,162,153,149,149,152,154,154,144,153,159,153,142,141,152,165,154,159,167,172,175,177,178,179,179,179,178,183,195,206,204,195,201,201,207,205,196,203,210,200,211,204,200,202,207,203,177,176,168,160,160,156,152,154,158,159,154,148,140,146,151,149,144,145,154,163,162,166,171,175,176,173,169,166,171,170,169,173,185,194,189,179,184,183,189,188,186,202,213,204,207,201,197,201,209,211,168,159,162,170,158,159,160,160,158,153,146,142,137,131,128,136,150,157,152,144,135,139,148,158,168,174,177,177,189,190,187,184,185,189,187,182,192,188,189,183,179,194,199,183,200,203,208,212,214,214,172,172,173,175,174,176,175,168,157,148,145,146,138,142,146,147,145,141,138,137,142,143,146,151,157,161,163,163,173,178,179,171,167,171,177,180,172,171,177,179,185,207,215,198,190,201,214,216,210,205])\n",
        "\n",
        "l = list.reshape((74 , 50))\n",
        "\n",
        "img = Image.fromarray(np.uint8(l))\n",
        "\n",
        "img\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4yc1zNF86d7x",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "836cb266-b94f-448d-9ee0-54bb5bc99e54"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=L size=50x74 at 0x7F154B851FD0>"
            ],
            "image/png": "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\n"
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instructions"
      ],
      "metadata": {
        "id": "pix_2GGLD1d6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## convert list into a tensor with name vec. Convert dtype to torch.float\n",
        "\n",
        "\n",
        "## resize vec into a 1D vector with name vec\n",
        "\n",
        "\n",
        "## Create a variable N which is the length of vector\n",
        "\n",
        "\n",
        "## We will now make a matrix called mat of size N * N such that A_{ij} = cos(-2 * pi * i * j / N). The instructions for \n",
        "## creating mat are given below:\n",
        "\n",
        "# First create a tensor named mat such that element in row i and column j is i*j. \n",
        "# Make sure you convert the dtype to torch.float\n",
        "\n",
        "\n",
        "# multiply mat by the constant -2 * pi / N\n",
        "\n",
        "\n",
        "# take the cosine of mat for the real part. Let the matrix remain in mat\n",
        "\n",
        "\n",
        "# transpose our vector, let it be stored within vec\n",
        "\n",
        "\n",
        "# multiply vec and mat. Store the result in DFT\n",
        "\n",
        "\n",
        "# print DFT\n"
      ],
      "metadata": {
        "id": "1a8o7XaADy8J"
      },
      "execution_count": 25,
      "outputs": []
    }
  ]
}